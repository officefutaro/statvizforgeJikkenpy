# ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° è©³ç´°ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ æ¦‚è¦

ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®**åŸºæœ¬çµ±è¨ˆæƒ…å ±**ã€**å“è³ª**ã€**æ§‹é€ **ã‚’ä½“ç³»çš„ã«èª¿æŸ»ãƒ»è©•ä¾¡ã™ã‚‹æ¢ç´¢çš„åˆ†æã®åŸºç¤æ‰‹æ³•ã§ã™ã€‚ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®å…¨ä½“åƒã‚’ç†è§£ã—ã€é©åˆ‡ãªåˆ†ææˆ¦ç•¥ã‚’æ±ºå®šã™ã‚‹ãŸã‚ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚

### é‡è¦æ€§
- ãƒ‡ãƒ¼ã‚¿å“è³ªã®æ—©æœŸç™ºè¦‹ã«ã‚ˆã‚Šã€åˆ†æç²¾åº¦å‘ä¸Š
- é©åˆ‡ãªå‰å‡¦ç†æ‰‹æ³•ã®é¸æŠæŒ‡é‡
- åˆ†æå·¥æ•°ã®å‰Šæ¸›ã¨åŠ¹ç‡åŒ–
- ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®åˆ†ã‹ã‚Šã‚„ã™ã„ç¾çŠ¶å ±å‘Š

## ğŸ¯ ä½¿ç”¨å ´é¢

### é©ç”¨ã‚·ãƒŠãƒªã‚ª
- **æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå–å¾—æ™‚**: åˆã‚ã¦æ‰±ã†ãƒ‡ãƒ¼ã‚¿ã®å…¨ä½“æŠŠæ¡
- **ãƒ‡ãƒ¼ã‚¿çµ±åˆå‰**: è¤‡æ•°ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®å“è³ªç¢ºèª
- **å®šæœŸçš„ãªå“è³ªç›£è¦–**: ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯
- **åˆ†æä¼ç”»æ™‚**: åˆ†æå¯èƒ½æ€§ã®äº‹å‰è©•ä¾¡

### ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥æ¨å¥¨åº¦
- **æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ (CSV, Excel)**: â˜…â˜…â˜…â˜…â˜… å¿…é ˆ
- **æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿**: â˜…â˜…â˜…â˜…â˜† é«˜æ¨å¥¨
- **ã‚«ãƒ†ã‚´ãƒªãƒ‡ãƒ¼ã‚¿**: â˜…â˜…â˜…â˜…â˜† é«˜æ¨å¥¨
- **ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: â˜…â˜…â˜…â˜†â˜† åŸºæœ¬çµ±è¨ˆã®ã¿
- **ç”»åƒãƒ»éŸ³å£°ãƒ‡ãƒ¼ã‚¿**: â˜…â˜…â˜†â˜†â˜† ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿

## ğŸ”§ å®Ÿè£…æ‰‹é †

### 1. ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨åŸºæœ¬æƒ…å ±ç¢ºèª

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('your_dataset.csv')

# åŸºæœ¬æƒ…å ±ã®ç¢ºèª
print("=" * 50)
print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåŸºæœ¬æƒ…å ±")
print("=" * 50)
print(f"è¡Œæ•°: {len(df):,}")
print(f"åˆ—æ•°: {len(df.columns)}")
print(f"ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print(f"ãƒ‡ãƒ¼ã‚¿å‹: {df.dtypes.value_counts().to_dict()}")
```

### 2. ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡

```python
def analyze_data_quality(df):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªã®åŒ…æ‹¬çš„è©•ä¾¡"""
    quality_report = {}
    
    for column in df.columns:
        col_data = df[column]
        
        # åŸºæœ¬çµ±è¨ˆ
        quality_report[column] = {
            'dtype': str(col_data.dtype),
            'non_null_count': col_data.count(),
            'null_count': col_data.isnull().sum(),
            'null_percentage': (col_data.isnull().sum() / len(df)) * 100,
            'unique_count': col_data.nunique(),
            'duplicate_count': col_data.duplicated().sum(),
        }
        
        # æ•°å€¤å‹ã®å ´åˆã®è¿½åŠ çµ±è¨ˆ
        if pd.api.types.is_numeric_dtype(col_data):
            quality_report[column].update({
                'mean': col_data.mean(),
                'median': col_data.median(),
                'std': col_data.std(),
                'min': col_data.min(),
                'max': col_data.max(),
                'q25': col_data.quantile(0.25),
                'q75': col_data.quantile(0.75),
                'outliers_iqr': detect_outliers_iqr(col_data),
                'skewness': stats.skew(col_data.dropna()),
                'kurtosis': stats.kurtosis(col_data.dropna())
            })
    
    return quality_report

def detect_outliers_iqr(data):
    """IQRæ³•ã«ã‚ˆã‚‹å¤–ã‚Œå€¤æ¤œå‡º"""
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = data[(data < lower_bound) | (data > upper_bound)]
    return len(outliers)

# å®Ÿè¡Œ
quality_report = analyze_data_quality(df)
```

### 3. çµ±è¨ˆçš„è¦ç´„ã®ç”Ÿæˆ

```python
def generate_statistical_summary(df):
    """çµ±è¨ˆçš„è¦ç´„ã®ç”Ÿæˆ"""
    summary = {
        'numerical_summary': df.describe(),
        'categorical_summary': df.describe(include=['object']),
        'correlation_matrix': df.corr(),
        'missing_data_pattern': df.isnull().sum().sort_values(ascending=False)
    }
    
    return summary

# å®Ÿè¡Œä¾‹
summary = generate_statistical_summary(df)
print("æ•°å€¤å¤‰æ•°ã®çµ±è¨ˆçš„è¦ç´„:")
print(summary['numerical_summary'])
```

### 4. å¯è¦–åŒ–ã«ã‚ˆã‚‹æ¢ç´¢

```python
def create_data_profile_plots(df, output_dir='./plots/'):
    """ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ç”¨ã®å¯è¦–åŒ–ä½œæˆ"""
    import os
    os.makedirs(output_dir, exist_ok=True)
    
    # 1. æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¯è¦–åŒ–
    plt.figure(figsize=(12, 6))
    sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
    plt.title('æ¬ æå€¤ãƒ‘ã‚¿ãƒ¼ãƒ³')
    plt.savefig(f'{output_dir}/missing_pattern.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # 2. æ•°å€¤å¤‰æ•°ã®åˆ†å¸ƒ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        fig, axes = plt.subplots(nrows=(len(numeric_cols)+2)//3, ncols=3, 
                                figsize=(15, 5*((len(numeric_cols)+2)//3)))
        axes = axes.flatten() if len(numeric_cols) > 1 else [axes]
        
        for i, col in enumerate(numeric_cols):
            if i < len(axes):
                sns.histplot(df[col].dropna(), kde=True, ax=axes[i])
                axes[i].set_title(f'{col} ã®åˆ†å¸ƒ')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/numeric_distributions.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    # 3. ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®åˆ†å¸ƒ
    categorical_cols = df.select_dtypes(include=['object']).columns
    if len(categorical_cols) > 0:
        for col in categorical_cols[:5]:  # æœ€åˆã®5ã¤ã®ã¿è¡¨ç¤º
            plt.figure(figsize=(10, 6))
            value_counts = df[col].value_counts().head(10)
            sns.barplot(x=value_counts.values, y=value_counts.index)
            plt.title(f'{col} ã®å€¤åˆ†å¸ƒï¼ˆä¸Šä½10ä½ï¼‰')
            plt.xlabel('å‡ºç¾å›æ•°')
            plt.savefig(f'{output_dir}/categorical_{col}.png', dpi=300, bbox_inches='tight')
            plt.show()

# å®Ÿè¡Œ
create_data_profile_plots(df)
```

### 5. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ

```python
def generate_data_profile_report(df, quality_report, output_file='data_profile_report.md'):
    """Markdownãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    
    report_content = f"""# ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ¦‚è¦
- **ãƒ•ã‚¡ã‚¤ãƒ«å**: {df.name if hasattr(df, 'name') else 'Unknown'}
- **è¡Œæ•°**: {len(df):,}
- **åˆ—æ•°**: {len(df.columns)}
- **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB
- **åˆ†ææ—¥æ™‚**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}

## ãƒ‡ãƒ¼ã‚¿å“è³ªã‚µãƒãƒªãƒ¼

### æ¬ æå€¤ã®çŠ¶æ³
"""
    
    # æ¬ æå€¤æƒ…å ±ã®è¿½åŠ 
    missing_summary = df.isnull().sum()
    missing_columns = missing_summary[missing_summary > 0]
    
    if len(missing_columns) > 0:
        report_content += "| åˆ—å | æ¬ ææ•° | æ¬ æç‡ |\n|------|--------|--------|\n"
        for col, missing_count in missing_columns.items():
            missing_rate = (missing_count / len(df)) * 100
            report_content += f"| {col} | {missing_count:,} | {missing_rate:.2f}% |\n"
    else:
        report_content += "âœ… **æ¬ æå€¤ãªã—** - å…¨ã¦ã®åˆ—ã§ãƒ‡ãƒ¼ã‚¿ãŒå®Œå…¨ã§ã™ã€‚\n"
    
    # æ•°å€¤å¤‰æ•°ã®ã‚µãƒãƒªãƒ¼
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        report_content += f"\n### æ•°å€¤å¤‰æ•°ã‚µãƒãƒªãƒ¼ ({len(numeric_cols)}åˆ—)\n\n"
        report_content += "| åˆ—å | å¹³å‡ | ä¸­å¤®å€¤ | æ¨™æº–åå·® | æœ€å°å€¤ | æœ€å¤§å€¤ | å¤–ã‚Œå€¤ |\n"
        report_content += "|------|------|--------|----------|--------|--------|---------|\n"
        
        for col in numeric_cols:
            col_data = df[col]
            outliers = detect_outliers_iqr(col_data)
            report_content += f"| {col} | {col_data.mean():.2f} | {col_data.median():.2f} | {col_data.std():.2f} | {col_data.min():.2f} | {col_data.max():.2f} | {outliers} |\n"
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_content)
    
    print(f"ãƒ¬ãƒãƒ¼ãƒˆã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    return report_content

# å®Ÿè¡Œ
report = generate_data_profile_report(df, quality_report)
```

## ğŸ“Š çµæœã®è§£é‡ˆ

### é‡è¦ãªæŒ‡æ¨™ã®èª­ã¿æ–¹

#### 1. ãƒ‡ãƒ¼ã‚¿å“è³ªæŒ‡æ¨™
- **æ¬ æç‡**: 10%ä»¥ä¸‹ãŒç†æƒ³ã€20%ä»¥ä¸Šã¯è¦æ³¨æ„
- **é‡è¤‡ç‡**: 5%ä»¥ä¸‹ãŒä¸€èˆ¬çš„ã€æ¥­å‹™ãƒ‡ãƒ¼ã‚¿ã§ã¯è¦ç¢ºèª
- **å¤–ã‚Œå€¤**: IQRæ³•ã§3%ä»¥ä¸‹ãŒé€šå¸¸ç¯„å›²

#### 2.åˆ†å¸ƒç‰¹æ€§
- **æ­ªåº¦ (Skewness)**:
  - -0.5 ï½ 0.5: æ­£è¦åˆ†å¸ƒã«è¿‘ã„
  - |0.5| ï½ |1.0|: è»½åº¦ã®æ­ªã¿
  - |1.0| ä»¥ä¸Š: å¼·ã„æ­ªã¿ï¼ˆå¯¾æ•°å¤‰æ›ç­‰ã‚’æ¤œè¨ï¼‰

- **å°–åº¦ (Kurtosis)**:
  - 3ä»˜è¿‘: æ­£è¦åˆ†å¸ƒ
  - 3ã‚ˆã‚Šå¤§ãã„: å°–ã£ãŸåˆ†å¸ƒ
  - 3ã‚ˆã‚Šå°ã•ã„: å¹³å¦ãªåˆ†å¸ƒ

#### 3. ç›¸é–¢é–¢ä¿‚
- **|r| > 0.7**: å¼·ã„ç›¸é–¢ï¼ˆå¤šé‡å…±ç·šæ€§ã«æ³¨æ„ï¼‰
- **0.3 < |r| < 0.7**: ä¸­ç¨‹åº¦ã®ç›¸é–¢
- **|r| < 0.3**: å¼±ã„ç›¸é–¢

### æ³¨æ„ç‚¹ãƒ»åˆ¶é™äº‹é …
- å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ï¼ˆ>100ä¸‡è¡Œï¼‰ã§ã¯å‡¦ç†æ™‚é–“ãŒé•·ããªã‚‹å¯èƒ½æ€§
- ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã®ç¨®é¡ãŒå¤šã„å ´åˆã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«æ³¨æ„
- æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã§ã¯æ™‚é–“è»¸ã®è€ƒæ…®ãŒå¿…è¦
- æ¬ æå€¤ã®æ„å‘³ï¼ˆæ§‹é€ çš„æ¬ æ vs ãƒ©ãƒ³ãƒ€ãƒ æ¬ æï¼‰ã®ç†è§£ãŒé‡è¦

### ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
1. **æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: å…¨ä½“ â†’ è©³ç´°ã®é †ã§åˆ†æ
2. **ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®æ´»ç”¨**: æ¥­å‹™ç†è§£ã¨çµ±è¨ˆåˆ†æã®çµ„ã¿åˆã‚ã›
3. **å¯è¦–åŒ–é‡è¦–**: æ•°å€¤ã ã‘ã§ãªãå›³è¡¨ã§ã®ç¢ºèª
4. **æ–‡æ›¸åŒ–**: ç™ºè¦‹äº‹é …ã®è¨˜éŒ²ã¨å…±æœ‰
5. **åå¾©çš„æ”¹å–„**: åˆ†æã‚’é€²ã‚ãªãŒã‚‰ç†è§£ã‚’æ·±åŒ–

## ğŸ“ˆ æ´»ç”¨ä¾‹

### ä¸å‹•ç”£ãƒ‡ãƒ¼ã‚¿ã®ä¾‹
```python
# å¸‚å·å¸‚ã‚¢ãƒ‘ãƒ¼ãƒˆå®¶è³ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ä¾‹
rent_df = pd.read_csv('ichikawa_apartment_rent.csv')

# ç‰¹å¾´çš„ãªåˆ†æãƒã‚¤ãƒ³ãƒˆ
print("å®¶è³ƒãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´:")
print(f"å¹³å‡å®¶è³ƒ: {rent_df['rent'].mean():.0f}å††")
print(f"å®¶è³ƒã®æ¨™æº–åå·®: {rent_df['rent'].std():.0f}å††")
print(f"ç¯‰å¹´æ•°ã¨å®¶è³ƒã®ç›¸é–¢: {rent_df['age'].corr(rent_df['rent']):.3f}")

# åœ°åŸŸåˆ¥ã®å®¶è³ƒåˆ†å¸ƒ
location_stats = rent_df.groupby('location')['rent'].agg(['mean', 'median', 'count'])
print("\nåœ°åŸŸåˆ¥å®¶è³ƒçµ±è¨ˆ:")
print(location_stats.sort_values('mean', ascending=False))
```

## ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯

### å†…éƒ¨é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [åˆæœŸæ¢ç´¢](../initial-exploration/ja.md): ã‚ˆã‚Šè©³ç´°ãªæ¢ç´¢æ‰‹æ³•
- [ç›¸é–¢åˆ†æ](../../statistical/correlation-analysis/ja.md): å¤‰æ•°é–“é–¢ä¿‚ã®åˆ†æ
- [å¯è¦–åŒ–ã‚¬ã‚¤ãƒ‰](../../visualization/chart-selection/ja.md): é©åˆ‡ãªã‚°ãƒ©ãƒ•ã®é¸æŠ

### å¤–éƒ¨å‚è€ƒè³‡æ–™
- [pandaså…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pandas.pydata.org/docs/)
- [seabornçµ±è¨ˆå¯è¦–åŒ–](https://seaborn.pydata.org/)
- [scipyçµ±è¨ˆé–¢æ•°](https://docs.scipy.org/doc/scipy/reference/stats.html)

### æ¨å¥¨ãƒ„ãƒ¼ãƒ«
- **Jupyter Notebook**: ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–åˆ†æ
- **pandas-profiling**: è‡ªå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- **Great Expectations**: ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–
- **Apache Superset**: ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä½œæˆ

---

**ä½œæˆæ—¥**: 2025å¹´1æœˆ30æ—¥  
**æœ€çµ‚æ›´æ–°**: 2025å¹´1æœˆ30æ—¥  
**ä½œæˆè€…**: Claude Code  
**ãƒ¬ãƒ“ãƒ¥ãƒ¼è€…**: StatVizForge ãƒ‡ãƒ¼ã‚¿åˆ†æãƒãƒ¼ãƒ 